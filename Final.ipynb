{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9QzFFW9mukc",
        "outputId": "8fbd1e96-dc89-4e27-a88a-185c1706123c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.309-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.40 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.309 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Samples to test.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrhDPGSHnrtW",
        "outputId": "3a22b85a-e16d-4758-d4c4-6cb570ef75b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Samples to test.zip\n",
            " extracting: 1 Clevedon Road, Hurstville NSW 2220.pdf  \n",
            " extracting: 1 ROBINSON ST CROYDON NSW 2132.pdf  \n",
            " extracting: 1, 218-220 Pacific Hwy, Greenwich, New South Wales 2065.pdf  \n",
            " extracting: 5, 76-80 BELGRAVE ST CREMORNE NSW 2090.pdf  \n",
            " extracting: 7 Empress Avenue, Rouse Hill NSW 2155.pdf  \n",
            " extracting: 8, 11-15 Young Street, Paddington, New South Wales 2021.pdf  \n",
            " extracting: 10 Phillips Street, Cabarita NSW 2137.pdf  \n",
            " extracting: 12 Francis Street, Hunters Hill, New South Wales 2110.pdf  \n",
            " extracting: 12 RYDE RD GORDON NSW 2072.pdf  \n",
            " extracting: 14 Orana Crescent, Blakehurst NSW 2221.pdf  \n",
            " extracting: 14 Patrick Street, Beacon Hill, New South Wales 2100.pdf  \n",
            " extracting: 16 Bridge Street Erskineville NSW 2043.pdf  \n",
            " extracting: 40, 20 Illawong Avenue TAMARAMA NSW 2026.pdf  \n",
            " extracting: 53 Alfred Street, Rozelle NSW 2039.pdf  \n",
            " extracting: 66 Clower Avenue, Rouse Hill NSW 2155.pdf  \n",
            " extracting: 71 Metropolitan Road, Enmore, New South Wales 2042.pdf  \n",
            " extracting: 92 & 92A Hoyle Drive, Dean Park, NSW 2761.pdf  \n",
            " extracting: 101 BARDWELL RD BARDWELL PARK NSW 2207.pdf  \n",
            " extracting: 116 Forrester Road, St Marys, New South Wales 2760.pdf  \n",
            " extracting: 122a Sanctuary Point Road, Sanctuary Point NSW 2540.pdf  \n",
            " extracting: 137 Garden Street, North Narrabeen, New South Wales 2101.pdf  \n",
            " extracting: 307, 8 Shout Ridge, Lindfield NSW 2070.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IRetrive:\n",
        "  def __init__(self, filepath):\n",
        "    loader = PyPDFLoader(filepath)\n",
        "    pages = loader.load_and_split()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "    texts = text_splitter.split_text(context)\n",
        "    st=\"\"\n",
        "    for i in texts:\n",
        "      st += i\n",
        "\n",
        "    sents = st.split(\"\\n\")\n",
        "    import re\n",
        "    pattern = r'land\\s*\\(\\s*Address\\s*,\\s*plan\\s*details\\s*and\\s*title\\s*reference\\s*\\)'\n",
        "    extracted_text=[]\n",
        "\n",
        "# The string to search in (including variations)\n",
        "    input_string = st  # Replace 'st' with your actual input string\n",
        "\n",
        "    # Use re.search with re.DOTALL flag to find a match across multiple lines\n",
        "    match = re.search(pattern, input_string, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        # Find the starting index of the match\n",
        "        # Find the line number where the match starts\n",
        "        start_line = input_string.count('\\n', 0, match.start()) + 1  # Add 1 to convert from 0-based index\n",
        "\n",
        "        # Split the input string into lines\n",
        "        lines = input_string.split('\\n')\n",
        "\n",
        "        # Extract the two lines after the match\n",
        "        extracted_lines = lines[start_line + 1 : start_line + 3]  # Extract lines 2 and 3 after the match\n",
        "\n",
        "        if extracted_lines:\n",
        "\n",
        "            # Create a string to store the extracted lines\n",
        "            extracted_text = \"\\n\".join([line.strip() for line in extracted_lines])\n",
        "            first_digit_index = re.search(r'\\d', extracted_text)\n",
        "\n",
        "            if first_digit_index:\n",
        "                # Trim the text to keep everything after the first digit\n",
        "                extracted_text = extracted_text[first_digit_index.start():]\n",
        "\n",
        "    keys = ['vendor  ','date for completion', 'land (address', 'plan details and', 'VACANT POSSESSION','purchaser’s solicitor    ','improvements','inclusions ','exclusions','Land tax']\n",
        "    self.st=st\n",
        "    dc={}\n",
        "    for item in keys:\n",
        "      for i,x in enumerate(sents):\n",
        "        if item in x:\n",
        "          dc[item] = [sents[i], sents[i+1], sents[i+2], sents[i+3],sents[i+4]]\n",
        "        else:\n",
        "          dc[item]=\" \"\n",
        "    pattern_v = re.compile(r'\\bvendor\\b\\s+([\\w\\s]+)', re.IGNORECASE)\n",
        "    matches_v = pattern_v.findall(st)\n",
        "    pattern_b = re.compile(r'\\bbalance\\b\\s*(\\$[\\d,.]+)')\n",
        "    matches_b = pattern_b.findall(st)\n",
        "    dc['purchaser’s solicitor    ']=matches_b\n",
        "    dc['vendor  '] = matches_v[0].split('\\n')\n",
        "    dc[keys[1]] = dc[keys[1]][0] #date of completion\n",
        "    dc['VACANT POSSESSION'] = dc['VACANT POSSESSION'][:-2]\n",
        "    #dc[keys[5]] = dc[keys[5]][-2]\n",
        "    dc['improvements'] = dc['improvements'][0:2]\n",
        "    dc['land (address']=extracted_text\n",
        "    dc['exclusions']=dc['exclusions'][0]\n",
        "\n",
        "    dc['Land tax']=dc['Land tax'][0]\n",
        "\n",
        "\n",
        "    self.dc = dc\n",
        "\n",
        "\n",
        "  def getName(self):\n",
        "      pattern = re.compile(r'\\bvendor\\b\\s+([\\w\\s]+)', re.IGNORECASE)\n",
        "      matches = pattern.findall(self.st)\n",
        "      vendor = matches[0].split('\\n')\n",
        "      if len(vendor) > 1:\n",
        "        return vendor[0]\n",
        "      else:\n",
        "        return \"Error\"\n",
        "\n",
        "  def getLandaddress(self):\n",
        "    return self.dc['land (address'].split(\"\\n\")[0]\n",
        "\n",
        "  def getPlandetails(self):\n",
        "    return self.dc['land (address'].split(\"\\n\")[1]\n",
        "  def getSettlementdate(self):\n",
        "    doc = self.dc['date for completion'].split('(',1)\n",
        "    if len(doc) == 0:\n",
        "      return \"Need to be confirmed\"\n",
        "    return doc\n",
        "\n",
        "  def getLandstatus(self):\n",
        "    sts = re.findall(r'\\uf0fe\\s*([^☐\\uf0fe]+)', self.dc['VACANT POSSESSION'][0])\n",
        "    if len(sts) == 0:\n",
        "      return \"Further clarification is also required of whether the Vendor will be able to provide vacant possession on settlement.\"\n",
        "    return sts\n",
        "\n",
        "  def getPrice(self):\n",
        "    pattern = re.compile(r'\\bbalance\\b\\s*(\\$[\\d,.]+)')\n",
        "    matches = pattern.findall(self.st)\n",
        "    print(matches)\n",
        "    #price = dc['purchaser’s solicitor    '].split('balance')[1].strip()\n",
        "    if len(matches) == 0:\n",
        "      return \"TBA\"\n",
        "    return matches[0]\n",
        "\n",
        "  def getImprovments(self):\n",
        "    selected_options = re.findall(r'\\uf0fe\\s*([^☐\\uf0fe]+)', self.dc['improvements'][0])\n",
        "\n",
        "    if len(selected_options) == 0:\n",
        "      return \"Need to be confirmed\"\n",
        "    elif len(selected_options) == 1:\n",
        "      return selected_options[0]\n",
        "    else:\n",
        "      impvs = \"\"\n",
        "      for i in range(0, len(selected_options)-1):\n",
        "        impvs += selected_options[i] + ','\n",
        "      impvs += 'and' + selected_options[-1]\n",
        "      return impvs\n",
        "\n",
        "  def getInclusions(self):\n",
        "    inc = re.findall(r'\\uf0fe\\s*([^☐\\uf0fe]+)', self.dc['inclusions '][0])\n",
        "\n",
        "    if len(inc) != 0:\n",
        "      return \"Inclusions are marked under the inclusion tab of the contract\"\n",
        "    return \"Inclusions are not marked under the inclusion tab of the contract\"\n",
        "\n",
        "  def getExclusions(self):\n",
        "    exc = self.dc['exclusions'].split('exclusions')[1]\n",
        "    return exc\n",
        "\n",
        "  def getLandtax(self):\n",
        "    ltx = re.findall(r'\\uf0fe\\s*([^☐\\uf0fe]+)', self.dc['Land tax'])\n",
        "    if len(ltx) == 0:\n",
        "      return \"Land tax is not marked as adjustable or not adjustable\"\n",
        "    elif ltx[0].strip().lower() == 'no':\n",
        "      return \"Land tax is marked as not adjustable\"\n",
        "    else:\n",
        "      return \"Land tax is marked as adjustable\"\n"
      ],
      "metadata": {
        "id": "fatkC36WSmV-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCuofS0-rUMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "i=0\n",
        "newData = {}\n",
        "for pdf in pdf_files[1:]:\n",
        "\n",
        "  print(i)\n",
        "  k=IRetrive(pdf)\n",
        "  i+=1\n",
        "\n",
        "  print(k.getName())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQXPf4wQqby4",
        "outputId": "5eacdded-9ebe-4671-81de-b09061ee925a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2521, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2628, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yan Yue and Hao Liang  \n",
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2719, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1400, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2695, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2964, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1467, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2756, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chen Chen and Oliver Slocombe Hulett  \n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2363, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2439, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JOHN ANTHONY MACPHERSON and JESSICA LORENA BRIZUELA\n",
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2318, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2437, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAN PETER BEKIARIS\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2647, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2633, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n",
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2643, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2566, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beverley Jane Milson  \n",
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2495, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1026, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2627, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cui Family Investment Pty Limited  \n",
            "8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2720, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1563, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1516, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2719, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lois Amy Russ and Carol Ann Appleby  \n",
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2648, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2629, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Siretta Lee Dawson  \n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2425, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2477, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feeras Al Jobory\n",
            "11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 3044, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1093, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2605, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n",
            "12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2286, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2447, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LI LIU\n",
            "13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2715, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1403, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2694, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sebastian Oreb and Felicia Oreb  \n",
            "14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2566, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1399, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2698, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zofia Helena Inwald and Graham Edgar Thorburn  \n",
            "15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2377, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1402, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2696, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nicholas John Roland Holman  \n",
            "16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2670, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2633, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error\n",
            "17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2357, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1005, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2455, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIRONMANI VAN GORKOM\n",
            "18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2682, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1093, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2605, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2413, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2514, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sonja Joy Hall and Michael Craig Hall  \n",
            "19\n",
            "Error\n",
            "20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 2561, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1345, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2605, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ying Zhuang  \n"
          ]
        }
      ]
    }
  ]
}